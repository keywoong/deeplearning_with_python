{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-1. Deep learning for text and sequences",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3/TwJZRjDayIsWyWl4qfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keywoong/deeplearning_with_python/blob/main/6_1_Deep_learning_for_text_and_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InwgsrJdQfiL"
      },
      "source": [
        "# 6.1 Word-level one-hot encoding \n",
        "import numpy as np\n",
        "\n",
        "samples = ['The cat sat on the mat', 'The dog ate my homework']\n",
        "\n",
        "token_index = {}\n",
        "# 이 곳에 samples에 포함된 단어들을 dictionary형태로 저장해줄 것이다.\n",
        "\n",
        "# samples에 포함된 모든 단어들에게 고유한 정수 값들을 부여한다. 이 과정을 통해 list of integer이 생성이 되는 것이다.\n",
        "for sample in samples:\n",
        "    for word in sample.split(): # 이 줄을 통해 단어가 token임을 알 수 있다.\n",
        "        if word not in token_index: # sample의 단어들을 보다가 token_index에 없는 첫 단어를 만나게 된다면\n",
        "            token_index[word] = len(token_index) + 1 # token_index에 넣어준다. 첫 token_index의 값은 1부터 시작한다.\n",
        "\n",
        "print('>> This is token_index')\n",
        "print(token_index)\n",
        "\n",
        "max_length = 10\n",
        "# 각각의 샘플들이 가질 수 있는 최대 단어의 개수는 10개이다. \n",
        "\n",
        "results = np.zeros(shape = (len(samples), max_length,max(token_index.values()) + 1))\n",
        "# results 이곳이 바로 one-hot encoding의 결과물이다. 여기서 max(token_index.values())는 10이므로 shape은 (2,10,11)이다.\n",
        "\n",
        "# 이 과정을 통해 results 행렬에 해당 단어에 대한 값들을 넣어준다. \n",
        "# toeken_index를 보면 1부터 시작하기 때문에 배열의 첫 부분은 비어있는 것이다. \n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split())) [: max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i,j,index] = 1.\n",
        "\n",
        "print('>>This is results of one-hot encoding')\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55L2Zw41hVCH"
      },
      "source": [
        "# 6.2 Character-level one-hot encoding \n",
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "# printable은 ASCII 문자열의 조합이다.\n",
        "print('>> This is characters')\n",
        "print(characters)\n",
        "# character를 출력하면 다음과 같이 아스키코드들이 출력된다.\n",
        "\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "# 위에서 characters를 dictionary형태로 만들어 token_index에 넣어준다. 이는 sample들을 벡터화시킬 때 문자를 이용하는 것이 아닌 그 문자에 할당된 고유한 정수값을 이용하기 위함이다.\n",
        "# 이렇게 하여 총 100개의 아스키코드 문자들이 dictionary형태로 저장된다.\n",
        "# zip()은 동일한 개수로 이루어진 자료형을 묶어주는 역할을 하는 함수이다.\n",
        "\n",
        "print('>> This is token_index')\n",
        "print(token_index)\n",
        "\n",
        "max_length = 50\n",
        "# 한 sample당 50개 이상의 단어를 가질 수 없다.\n",
        "\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "# results는 shape이 (2, 50, 101)이다.\n",
        "\n",
        "# 이 과정을 통해 results 행렬에 해당 문자에 대한 값들을 넣어준다. \n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample):\n",
        "        index = token_index.get(characters)\n",
        "        results[i,j,index] = 1.\n",
        "\n",
        "# 이렇게 하여 results를 출력하면 \n",
        "print('>> This is results of one-hot encoding')\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH1ZaxzAlKFW"
      },
      "source": [
        "# 6.3 Using keras for word-level one-hot encoding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "# sample들이 사용할 수 있는 최대 단어의 개수는 10개이다.\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "# 각각의 sample들에 포함된 단어들을 토큰화시킨다.\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "# 토큰화시킨 단어들의 정수값들을 바탕으로 sample들을 정수값으로 변환한다.\n",
        "\n",
        "print('>> This is sequences, 정수값들로 변환시킨 sample들이다.')\n",
        "print(sequences)\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary')\n",
        "# 정수값으로 변환시킨 샘플들을 one-hot encoding시킨다. \n",
        "# 벡터의 크기는 10이고, 이 samples의 단어의 총 개수는 9개이므로, 1부터 9까지 각 sample이 갖고 있는 단어에 1을 넣는다.\n",
        "\n",
        "print('>>This is one_hot_results')\n",
        "print(one_hot_results)\n",
        "print('Shape is ', one_hot_results.shape)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print('Found %s unique tokens. ' %len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XgO44gqvTL1"
      },
      "source": [
        "# 6.4 Word-level one-hot encoding with hashing trick\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "dimensionality = 1000\n",
        "# 차원이 클수록 해쉬충돌의 가능성은 낮아진다.\n",
        "max_length = 10\n",
        "# 한 샘플 당 10개 이상의 단어를 가질 수 없다.\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split())) [: max_length]:\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        #hash()라는 hashing function을 이용하여 sample의 word들을 hash한 값에 매핑시키고, 그 값에 절댓값을 씌워 정수값으로 지정한다.\n",
        "        results[i,j,index] = 1.\n",
        "\n",
        "print('The shape of results is ', results.shape)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7spK2nM8NvD"
      },
      "source": [
        "# 6.5 Instatiating an Embedding layer\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000,64)\n",
        "# 여기서 1000은 사용할 수 있는 최대 토큰(단어)의 개수이고, 64는 차원이다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cenQqjZ_C540"
      },
      "source": [
        "# 6.6 Loading the IMDB data for use with an Embedding layer\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "# 입력시키는 모든 문장들에 포함되는 모든 단어의 개수는 10000를 넘어서는 안된다.\n",
        "maxlen = 20\n",
        "# 각각의 문장은 20개 이하의 문장으로 구성된다.\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYS2dPHCG5tz"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igBtHBPVHEVZ"
      },
      "source": [
        "# 6.7 Using an Embedding layer and classifier on the IMDB data\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length = maxlen))\n",
        "# maxlen에 딱 맞추어서 input값을 입력받는다.\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCRKdPz3IDjI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9M0QDFCVi8J"
      },
      "source": [
        "# 6.8 Processing the labels of the raw IMDB data\n",
        "import os\n",
        "\n",
        "imdb_dir = '/content/drive/My Drive/deeplearning_with_python/kaggle/imdb/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []# labels 배열은 긍정인지, 부정인지를 판단\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    # 부정적인 리뷰 먼저, 그 다음 긍정적인 리뷰를 조사한다.\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):#폴더명이 neg인 경우, pos인 경우 나뉘어서 조사\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())# texts라는 배열에 텍스트형식의 파일내용들을 넣는다.\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "            # 100번째 파일을 읽은 후, 파일 내용들을 texts배열에 넣는다.(texts[100] = 파일 내용들)\n",
        "            # 그리고 이 파일이 부정적인 데이터라면 labels배열에 0을 넣는다.(labels[100] = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2ljAJwk5Hc0"
      },
      "source": [
        "print(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKQaLHwIaAEH"
      },
      "source": [
        "# 6.9 Tokenizing the text of the raw IMDB data\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100 # 각 리뷰 6.9 Tokenizing the text of the raw IMDB data\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100 # 각 리뷰 당 최대 100개의 단어까지 사용할 수 있다.\n",
        "training_samples = 200 # pretrained Word Embedding을 사용하므로 샘플 수를 200개로 제한한다.\n",
        "validation_samples = 10000 # 검증을 위한 샘플의 개수는 10000개로 설정한다.\n",
        "max_words = 10000 # 모든 샘플들은 10000개 이하의 단어를 사용해야 한다.(Consider only the top 10000 words in the dataset이라고 쓰여있음)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_words) # ???? 이게 뭘까~요?\n",
        "tokenizer.fit_on_texts(texts) # achllmdb에 있는 txt파일의 내용들을 담은 texts 배열의 내부 어휘들을 업데이트한다.\n",
        "sequences = tokenizer.texts_to_sequences(texts) # 각각의 텍스트를 정수 시퀀스로 변환한다.\n",
        "word_index = tokenizer.word_index # sequences들을 사전 형태로 만들어준다.\n",
        "# word_index['the'] = 1 / word_index['counterearth'] = 72633 와 같이 사전 형태로 되어 있다. \n",
        "\n",
        "\n",
        "# IMDB 샘플들을 입력받아 토큰화 시킨 결과, 총 72633개의 토큰이 나왔다.\n",
        "print('Found %s unique tokens' %len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-hIzwq0Pof-"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSw4VCYoQw3v"
      },
      "source": [
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "# 2차원(samples, timesteps) numpy 배열인 sequences를 pad_sequences 함수를 통해 길이가 같지 않은 배열들을 얼정한 길이 maxlen으로 맞춰줘 data에 넣는다.\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "# 0또는 1로 이루어져있던 정수 리스트 형태의 labels를 numpy 형태로 만들어준다.\n",
        "print(labels)\n",
        "print('The size of labels is %s' %len(labels))\n",
        "# 총 17261개의 txt파일을 읽었으므로 labels의 사이즈는 17261개이다.\n",
        "\n",
        "print('Shape of data tensor: ', data.shape)\n",
        "# 총 17261개의 텍스트 데이터들을 maxlen으로 제한했으므로 (17261,100)이다.\n",
        "print('Shape of label tensor: ', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "# arange 함수를 이용해 0부터 17260까지 배열하고 indices에 넣어준다. \n",
        "print(indices)\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "# 초기 상태의 데이터는 순서대로 배열되어있다. (부정 먼저 탐색했으므로 부정 먼저, 그 다음 긍정)\n",
        "# 이 데이터들을 섞어준다.\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# 데이터들을 train용, validation 용으로 나누어준다.\n",
        "x_train = data[:training_samples] # 0부터 200까지 train 데이터에 넣는다.\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples : training_samples + validation_samples] # 200 번째 데이터부터 10200번째 데이터까지 validation 데이터에 넣는다.\n",
        "y_val = labels[training_samples : training_samples + validation_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLFR-aODUe4W"
      },
      "source": [
        "# 6.10 Parsing the GloVe word-embeddings file\n",
        "\n",
        "glove_dir = '/content/drive/My Drive/deeplearning_with_python/kaggle/glove'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors' %len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IJP9IUIaVY6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}